![:wip](Under Construction)

class: title
## NPFL114, Lecture 04

# Convolutional Networks

![:pdf 26%,,padding-right:64px](../res/mff.pdf)
![:image 33%,,padding-left:64px](../res/ufal.svg)

.author[
Milan Straka
]

---
# Convergence

The training process might or might not converge. Even if it does, it might
converge slowly or quickly.

There are _many_ factors influencing convergence and its speed, we now discuss
three of them.

---
class: middle, full
# Convergence – Saturating Non-linearities

![:image 100%](tanh.png)

---
# Convergence – Parameter Initialization

Neural networks usually need random initialization to _break symmetry_.

- Biases are usually initialized to a constant value, usually 0.

--

- Weights are usually initialized to small random values, either with uniform or
  normal distribution.
   - The scale matters for deep networks!

   - Originally, people used $U\left[-\frac{1}{\sqrt n}, \frac{1}{\sqrt n}\right]$ distribution.

--

   - Xavier Glorot and Yoshua Bengio, 2010:
     _Understanding the difficulty of training deep feedforward neural networks_.

     Theoretically and experimentally showed that a suitable way to initialize a $ℝ^{n×m}$
     matrix is $$U\left[-\sqrt{\frac{6}{m+n}}, \sqrt{\frac{6}{m+n}}\right].$$

---
class: middle, center
# Convergence – Parameter Initialization

![:pdf 85%](glorot_activations.pdf)

---
class: middle, center
# Convergence – Parameter Initialization

![:pdf 85%](glorot_gradients.pdf)

---
class: middle
# Convergence – Gradient Clipping

![:pdf 100%](exploding_gradient.pdf)

---
# Convergence – Gradient Clipping

![:pdf 80%,center](gradient_clipping.pdf)

Using a given maximum norm, we may clip the gradient.
$$g ← \begin{cases}
  g & \textrm{ if }||g|| ≤ c \\\
  c \frac{g}{||g||} & \textrm{ if }||g|| > c
\end{cases}$$

The clipping can be per weight, per matrix or for the gradient as a whole.

---
class: middle, center
# Going Deeper

# Going Deeper

---
# Convolutional Networks

Consider data with some structure (temporal data, speech, images, …).

Unlike densely connected layers, we might want:
- Sparse (local) interactions
- Parameter sharing (equal response everywhere)
- Shift invariance

---
class: middle
# Convolutional Networks

![:image 100%](convolution.png)

---
# Convolution Operation

For a functions $x$ and $w$, _convolution_ $x \* w$ is defined as
$$(x \* w)(t) = ∫x(a)w(t - a)\d a.$$

For vectors, we have
$$(→x \* →w)\_t = ∑\_i x\_i w\_{t-i}.$$

Convolution operation can be generalized to two dimensions by
$$(⇉I \* ⇉K)\_{i, j} = ∑\_{m, n} ⇉I\_{m, n} ⇉K\_{i-m, j-n}.$$

Closely related is _cross-corellation_, where $K$ is flipped:
$$S\_{i, j} = ∑\_{m, n} ⇉I\_{i+m, j+n} ⇉K\_{m, n}.$$

---
class: center
# Convolution

![:pdf 74%](convolution_all.pdf)

---
# Convolution Operation

The $K$ is usually called a _kernel_ or a _filter_, and we generally apply
several of them at the same time.

Consider an input image with $C$ channels. The convolution operation with
$F$ filters of width $W$, height $H$ and stride $S$ produces an output with $F$ channels
kernels of total size $W × H × C × F$ and is computed as
$$(⇉I \* ⇉K)\_{i, j, k} = ∑\_{m, n, o} ⇉I\_{i\cdot s + m, j\cdot s + n, o} ⇉K\_{m, n, o, k}.$$

--

There are multiple padding schemes, most common are:
- `valid`: we only use valid pixels, which causes the result to me smaller
- `same`: we pad original image with zero pixels so that the result is exactly
  the size of the input

---
# Pooling

Pooling is an operation similar to convolution, but we perform a fixed operation
instead of multiplying by a kernel.

- Max pooling: minor translation invariance
- Average pooling

![:pdf 80%,center](pooling.pdf)

---
# High-level CNN Architecture

We repeatedly use the following block:
1. Convolution operation
2. Non-linear activation (usually ReLU)
3. Pooling

![:image 100%](cnn.jpg)

---
class: middle
# Similarities in V1 and CNNs

![:pdf 100%](gabor_functions.pdf)
The primary visual cortex recognizes Gabor functions.

---
class: middle
# Similarities in V1 and CNNs

![:pdf 100%](first_convolutional_layer.pdf)
Similar functions are recognized in the first layer of a CNN.

---
class: middle
# AlexNet

![:pdf 100%](alexnet.pdf)

---
class: middle, center
# AlexNet

![:pdf 85%](relu_vs_tanh.pdf)
